# Design: Multi-Model Redundancy for Analysis

## 1. Overview

This document outlines the design for extending the `analyze` command to support multiple LLM models. The goal is to increase reliability and provide diverse perspectives by running analyses redundantly (or sequentially) across different models (e.g., OpenAI GPT-4o and Google Gemini 2.0 via OpenRouter).

## 2. Configuration Changes

### 2.1 `LLMConfig` Update
The `LLMConfig` class in `src/macrolens_poc/config.py` will be updated to support a list of models instead of a single model string.

**Current:**
```python
class LLMConfig(BaseModel):
    model: str = Field(default="gpt-4-turbo")
    # ...
```

**New:**
```python
class LLMConfig(BaseModel):
    models: list[str] = Field(default_factory=lambda: ["gpt-4-turbo"])
    # ...
```

### 2.2 Environment Variables
We will introduce `LLM_MODELS` as a comma-separated list in `.env`.

*   `LLM_MODELS="openai/gpt-4o-mini,google/gemini-2.0-flash-exp:free"`

**Backward Compatibility:**
*   If `LLM_MODELS` is set, it takes precedence.
*   If `LLM_MODELS` is missing but `LLM_MODEL` is set, `models` will be initialized as `[LLM_MODEL]`.

## 3. Service Logic (`AnalysisService`)

The `AnalysisService` in `src/macrolens_poc/llm/service.py` will be refactored to iterate over the configured models.

### 3.1 Execution Flow
1.  Load report data and prompts (once).
2.  Iterate through `settings.llm.models`.
3.  For each model:
    *   Instantiate/Configure the Provider with the specific model.
    *   Call `generate_analysis`.
    *   Catch and log errors (Best Effort). If one model fails, continue with the next.
4.  Combine results into a single Markdown output.

### 3.2 Output Format
The output Markdown will contain sections for each model's analysis.

```markdown
# Market Analysis Report

[Metadata/Header info...]

## Analysis (openai/gpt-4o-mini)

[Content generated by GPT-4o...]

## Analysis (google/gemini-2.0-flash-exp:free)

[Content generated by Gemini...]
```

## 4. Provider Changes (`OpenAIProvider`)

The `OpenAIProvider` needs to know which model to use for a specific request, as `LLMConfig` now holds a list.

*   Update `OpenAIProvider.__init__` to accept an optional `model_name: str`.
*   If `model_name` is provided, use it for API calls.
*   If not provided, fallback to the first model in `config.models` (or raise error).

## 5. CLI Changes

The `analyze` command in `src/macrolens_poc/cli.py` will support a `--models` flag to override the configuration at runtime.

```bash
macrolens-poc analyze --report-file ... --output ... --models "openai/gpt-4o,anthropic/claude-3-opus"
```

## 6. Implementation Plan

1.  **Refactor Config**: Update `LLMConfig` and `load_settings` to handle `models` list and `LLM_MODELS` env var.
2.  **Update Provider**: Modify `OpenAIProvider` to accept a specific model string.
3.  **Update Service**: Implement the loop over models and error handling in `AnalysisService`.
4.  **Update CLI**: Add `--models` option and pass it to the service/settings.
5.  **Testing**: Verify with single model (backward compat) and multiple models.
